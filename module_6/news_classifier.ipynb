{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a6e1746-8787-4ca8-a472-978a76643557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_bbc_dataset(base_path='bbc'):\n",
    "    \"\"\"\n",
    "    Считывает все txt-файлы из подпапок папки base_path,\n",
    "    формирует список (текст, метка) и возвращает pd.DataFrame.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for label_folder in os.listdir(base_path):\n",
    "        folder_path = os.path.join(base_path, label_folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith('.txt'):\n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                    # Пробуем сначала открыть как utf-8, при ошибке — как latin-1\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            text = f.read()\n",
    "                    except UnicodeDecodeError:\n",
    "                        with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                            text = f.read()\n",
    "\n",
    "                    data.append((text, label_folder))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['text', 'label'])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c24cd6e-a80f-45e1-9e5b-ad3cea277356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
      "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
      "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
      "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
      "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business\n",
      "label\n",
      "sport            511\n",
      "business         506\n",
      "politics         417\n",
      "tech             401\n",
      "entertainment    386\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "base_path = \"bbc\"  # Путь к папке, где лежат подпапки business, entertainment и т.д.\n",
    "df = load_bbc_dataset(base_path)\n",
    "print(df.head())\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "502d2be8-258f-411a-8122-3afe29896270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: (1776, 2)\n",
      "Размер валидационной выборки: (222, 2)\n",
      "Размер тестовой выборки: (223, 2)\n"
     ]
    }
   ],
   "source": [
    "# 1. Разделение данных на Train (80%), Validation (10%) и Test (10%)\n",
    "# Предполагается, что исходный DataFrame с колонками 'clean_text' (очищенный текст) и 'label' уже подготовлен (см. предыдущий шаг)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.20, random_state=42, stratify=df['label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.50, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "print(\"Размер обучающей выборки:\", train_df.shape)\n",
    "print(\"Размер валидационной выборки:\", val_df.shape)\n",
    "print(\"Размер тестовой выборки:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cf7dffd-9d3b-41d6-80d6-09f002e6a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Функция очистки текста – стандартная предобработка (удаляет HTML, приводит к нижнему регистру, удаляет лишние пробелы, знаки препинания, стоп-слова и выполняет стемминг)\n",
    "def clean_text(text):\n",
    "    # 1. Удаление HTML-тегов\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # 2. Замена множественных пробелов, переносов строк и табуляций на один пробел\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # 3. Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # 4. Удаление символов, отличных от букв (оставляем только латинские буквы и пробелы)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # 5. Токенизация\n",
    "    tokens = word_tokenize(text)\n",
    "    # 6. Удаление стоп-слов\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # 7. Стемминг\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Объединение токенов в строку\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Класс, который объединяет всю цепочку: векторизацию, обучение классификатора и инференс.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "class NewsClassifier:\n",
    "    def __init__(self, ngram_range=(1, 2), min_df=5, C=1.0):\n",
    "        # Инициализируем TF-IDF векторизатор и классификатор Logistic Regression\n",
    "        self.vectorizer = TfidfVectorizer(ngram_range=ngram_range, min_df=min_df)\n",
    "        self.clf = LogisticRegression(random_state=42, max_iter=200, C=C)\n",
    "    \n",
    "    def fit(self, texts, labels):\n",
    "        \"\"\"Обучение модели на списке сырой текстов и меток. При этом каждый текст проходит очистку.\"\"\"\n",
    "        # Применяем функцию очистки к каждому тексту\n",
    "        cleaned_texts = [clean_text(text) for text in texts]\n",
    "        # Векторизуем очищенные тексты\n",
    "        X = self.vectorizer.fit_transform(cleaned_texts)\n",
    "        # Обучаем классификатор\n",
    "        self.clf.fit(X, labels)\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"\n",
    "        Принимает на вход один сырой текст новости,\n",
    "        выполняет очистку, векторизацию, и возвращает:\n",
    "            - предсказанную метку,\n",
    "            - словарь вероятностей по классам.\n",
    "        \"\"\"\n",
    "        cleaned = clean_text(text)\n",
    "        X = self.vectorizer.transform([cleaned])\n",
    "        label = self.clf.predict(X)[0]\n",
    "        probs = self.clf.predict_proba(X)[0]\n",
    "        probabilities = dict(zip(self.clf.classes_, probs))\n",
    "        return label, probabilities\n",
    "    \n",
    "    def predict_batch(self, texts):\n",
    "        \"\"\"\n",
    "        Предсказывает для списка текстов.\n",
    "        Возвращает:\n",
    "            - массив предсказанных меток,\n",
    "            - список словарей с вероятностями.\n",
    "        \"\"\"\n",
    "        cleaned_texts = [clean_text(text) for text in texts]\n",
    "        X = self.vectorizer.transform(cleaned_texts)\n",
    "        labels = self.clf.predict(X)\n",
    "        prob_array = self.clf.predict_proba(X)\n",
    "        prob_list = [dict(zip(self.clf.classes_, probs)) for probs in prob_array]\n",
    "        return labels, prob_list\n",
    "    \n",
    "    def evaluate(self, texts, true_labels):\n",
    "        \"\"\"Выводит отчет по метрикам (accuracy, classification report) на переданном наборе.\"\"\"\n",
    "        pred_labels, _ = self.predict_batch(texts)\n",
    "        print(classification_report(true_labels, pred_labels))\n",
    "        print(\"Accuracy:\", accuracy_score(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bab68ee1-7bde-4f56-baf4-7a740252b7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики на валидационной выборке:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       1.00      0.98      0.99        50\n",
      "entertainment       1.00      1.00      1.00        39\n",
      "     politics       0.98      0.98      0.98        42\n",
      "        sport       1.00      1.00      1.00        51\n",
      "         tech       0.98      1.00      0.99        40\n",
      "\n",
      "     accuracy                           0.99       222\n",
      "    macro avg       0.99      0.99      0.99       222\n",
      " weighted avg       0.99      0.99      0.99       222\n",
      "\n",
      "Accuracy: 0.990990990990991\n",
      "Метрики на тестовой выборке:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       1.00      1.00      1.00        51\n",
      "entertainment       1.00      0.97      0.99        38\n",
      "     politics       0.98      1.00      0.99        42\n",
      "        sport       1.00      1.00      1.00        52\n",
      "         tech       1.00      1.00      1.00        40\n",
      "\n",
      "     accuracy                           1.00       223\n",
      "    macro avg       1.00      0.99      0.99       223\n",
      " weighted avg       1.00      1.00      1.00       223\n",
      "\n",
      "Accuracy: 0.9955156950672646\n"
     ]
    }
   ],
   "source": [
    "# Инициализация модели (параметры можно подбирать через GridSearchCV отдельно)\n",
    "classifier = NewsClassifier(ngram_range=(1,2), min_df=5, C=1.0)\n",
    "# Обучаем на обучающей выборке (используем сырой текст, а очистка происходит внутри fit)\n",
    "classifier.fit(train_df['text'], train_df['label'])\n",
    "\n",
    "# Оценка на валидационной выборке\n",
    "print(\"Метрики на валидационной выборке:\")\n",
    "classifier.evaluate(val_df['text'], val_df['label'])\n",
    "\n",
    "# Итоговая оценка на тестовой выборке (эта выборка не трогалась при обучении)\n",
    "print(\"Метрики на тестовой выборке:\")\n",
    "classifier.evaluate(test_df['text'], test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9476cf0b-9bb6-4276-9d03-6c95b02dab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Пример инференса одного образца:\n",
      "Исходный текст: The government announced new policies in tech innovation to boost the local economy.\n",
      "Предсказанная метка: business\n",
      "Вероятности по классам:\n",
      "  business: 0.4136\n",
      "  entertainment: 0.1186\n",
      "  politics: 0.2331\n",
      "  sport: 0.1280\n",
      "  tech: 0.1066\n"
     ]
    }
   ],
   "source": [
    "# Пример инференса одного образца\n",
    "sample_text = \"The government announced new policies in tech innovation to boost the local economy.\"\n",
    "pred_label, pred_probs = classifier.predict(sample_text)\n",
    "\n",
    "print(\"\\nПример инференса одного образца:\")\n",
    "print(\"Исходный текст:\", sample_text)\n",
    "print(\"Предсказанная метка:\", pred_label)\n",
    "print(\"Вероятности по классам:\")\n",
    "for lbl, prob in pred_probs.items():\n",
    "    print(f\"  {lbl}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a79c5576-428c-4baa-81b6-0684930cc7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первичные данные нового датасета:\n",
      "                                                text true_label\n",
      "0  Investors facing tariff turmoil: 'It's fastest...   business\n",
      "1  Prada to buy rival fashion brand Versace for $...   business\n",
      "2  How exposed is the UK to Trump's tariff chaos?...   business\n",
      "3  UK economy grew more than expected in February...   business\n",
      "4  Watch: Why US markets skyrocketed after Trump ...   business\n",
      "\n",
      "Результаты инференса на новом датасете:\n",
      "      true_label     pred_label  \\\n",
      "0       business       business   \n",
      "1       business       business   \n",
      "2       business       business   \n",
      "3       business       business   \n",
      "4       business       business   \n",
      "5  entertainment  entertainment   \n",
      "6  entertainment  entertainment   \n",
      "7  entertainment  entertainment   \n",
      "8  entertainment  entertainment   \n",
      "9  entertainment  entertainment   \n",
      "\n",
      "                                           pred_prob  \n",
      "0  {'business': 0.7730855920339225, 'entertainmen...  \n",
      "1  {'business': 0.6826105821819952, 'entertainmen...  \n",
      "2  {'business': 0.7767271579857042, 'entertainmen...  \n",
      "3  {'business': 0.6646165821649229, 'entertainmen...  \n",
      "4  {'business': 0.5456373014680315, 'entertainmen...  \n",
      "5  {'business': 0.04923520673900425, 'entertainme...  \n",
      "6  {'business': 0.06111774766315568, 'entertainme...  \n",
      "7  {'business': 0.04121246661738126, 'entertainme...  \n",
      "8  {'business': 0.14857925854692453, 'entertainme...  \n",
      "9  {'business': 0.04999411394661076, 'entertainme...  \n",
      "\n",
      "Отчет по предсказаниям на новом датасете:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.45      1.00      0.62         5\n",
      "entertainment       1.00      1.00      1.00         5\n",
      "     politics       1.00      0.40      0.57         5\n",
      "        sport       1.00      1.00      1.00         5\n",
      "         tech       1.00      0.40      0.57         5\n",
      "\n",
      "     accuracy                           0.76        25\n",
      "    macro avg       0.89      0.76      0.75        25\n",
      " weighted avg       0.89      0.76      0.75        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_bbc_new_dataset(base_path='bbc_new'):\n",
    "    \"\"\"\n",
    "    Считывает все txt-файлы из подпапок в base_path.\n",
    "    В каждой подпапке имя соответствует метке.\n",
    "    Возвращает DataFrame с колонками 'text' и 'true_label'.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    # Проходим по каждой подпапке\n",
    "    for label_folder in os.listdir(base_path):\n",
    "        folder_path = os.path.join(base_path, label_folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Проходим по всем .txt файлам в подпапке\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith('.txt'):\n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            text = f.read()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Ошибка чтения файла {file_path}: {e}\")\n",
    "                        continue\n",
    "                    data.append((text, label_folder))\n",
    "    # Формируем DataFrame\n",
    "    df_new = pd.DataFrame(data, columns=['text', 'true_label'])\n",
    "    return df_new\n",
    "\n",
    "# Загружаем новый датасет из папки 'bbc_new'\n",
    "df_new = load_bbc_new_dataset(\"bbc_new\")\n",
    "print(\"Первичные данные нового датасета:\")\n",
    "print(df_new.head())\n",
    "\n",
    "# Предполагается, что вы обучили модель и сохранили объект classifier\n",
    "# Например: \n",
    "# classifier = NewsClassifier(ngram_range=(1,2), min_df=5, C=1.0)\n",
    "# classifier.fit(train_texts, train_labels)\n",
    "# Теперь мы будем использовать метод predict_batch для применения модели к набору новостей.\n",
    "\n",
    "# Получаем предсказания для всех текстов нового датасета\n",
    "pred_labels, pred_prob_list = classifier.predict_batch(df_new['text'])\n",
    "\n",
    "# Добавляем столбец с предсказаниями в DataFrame\n",
    "df_new['pred_label'] = pred_labels\n",
    "df_new['pred_prob'] = pred_prob_list\n",
    "\n",
    "# Выводим первые несколько строк с результатами\n",
    "print(\"\\nРезультаты инференса на новом датасете:\")\n",
    "print(df_new[['true_label', 'pred_label', 'pred_prob']].head(10))\n",
    "\n",
    "# Если для нового набора новостей известны истинные метки (как столбец 'true_label'),\n",
    "# можно вычислить метрики (например, classification_report)\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nОтчет по предсказаниям на новом датасете:\")\n",
    "print(classification_report(df_new['true_label'], df_new['pred_label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27c7f3b7-16cc-45c8-89d7-fc3710ddf906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7aa9a52555482ca375fb752c92c4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1776 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7963d9b440b04ba7acea80c78425c2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/222 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65eaae4e600498fa53a2ea422f077de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13576\\1604105455.py:95: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='666' max='666' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [666/666 1:28:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.300700</td>\n",
       "      <td>0.082336</td>\n",
       "      <td>0.977477</td>\n",
       "      <td>0.977928</td>\n",
       "      <td>0.977477</td>\n",
       "      <td>0.977454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>0.150252</td>\n",
       "      <td>0.977477</td>\n",
       "      <td>0.977475</td>\n",
       "      <td>0.977477</td>\n",
       "      <td>0.977314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.126898</td>\n",
       "      <td>0.981982</td>\n",
       "      <td>0.981982</td>\n",
       "      <td>0.981982</td>\n",
       "      <td>0.981928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 01:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0013865981018170714, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 62.374, 'eval_samples_per_second': 3.575, 'eval_steps_per_second': 0.449, 'epoch': 3.0}\n",
      "Исходный текст: A new movie starring popular actors is releasing soon.\n",
      "Предсказанный класс: entertainment\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers datasets accelerate -q\n",
    "\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ==== 1) Подготовка текстов и лейблов ====\n",
    "# У нас есть:\n",
    "#   train_df, val_df, test_df\n",
    "# со столбцами 'text' (или 'clean_text') и 'label'.\n",
    "# Для BERT лучше взять 'text' с минимальной предобработкой.\n",
    "\n",
    "label_list = df['label'].unique().tolist()   # ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "def encode_labels(label_str):\n",
    "    return label_to_id[label_str]\n",
    "\n",
    "train_df['label_id'] = train_df['label'].apply(encode_labels)\n",
    "val_df['label_id']   = val_df['label'].apply(encode_labels)\n",
    "test_df['label_id']  = test_df['label'].apply(encode_labels)\n",
    "\n",
    "# Превращаем pandas.DataFrame в huggingface Dataset\n",
    "train_ds = Dataset.from_pandas(train_df[['text', 'label_id']])\n",
    "val_ds   = Dataset.from_pandas(val_df[['text', 'label_id']])\n",
    "test_ds  = Dataset.from_pandas(test_df[['text', 'label_id']])\n",
    "\n",
    "train_ds = train_ds.rename_column(\"label_id\", \"labels\")\n",
    "val_ds = val_ds.rename_column(\"label_id\", \"labels\")\n",
    "test_ds = test_ds.rename_column(\"label_id\", \"labels\")\n",
    "\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_ds,\n",
    "    'validation': val_ds,\n",
    "    'test': test_ds\n",
    "})\n",
    "\n",
    "# ==== 2) Токенизация ====\n",
    "# Инициализируем DistilBert-токенизатор\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",      # или \"longest\", в зависимости от задачи\n",
    "        truncation=True,\n",
    "        max_length=256             # подберите по своим ресурсам и данным\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# ==== 3) Создание модели DistilBERT ====\n",
    "# Число классов = 5 (business, entertainment, politics, sport, tech)\n",
    "num_labels = 5\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "# ==== 4) Метрики ====\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# ==== 5) Настройки обучения ====\n",
    "batch_size = 8\n",
    "logging_steps = len(tokenized_dataset['train']) // batch_size\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distilbert-bbc\",       \n",
    "    eval_strategy=\"epoch\",       # или \"steps\"\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=5e-5,\n",
    "    load_best_model_at_end=True,       # чтобы иметь лучшую модель в конце\n",
    "    metric_for_best_model=\"accuracy\",  # какую метрику использовать при сохранении best\n",
    ")\n",
    "\n",
    "# ==== 6) Trainer ====\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ==== 7) Запуск обучения (fine-tuning DistilBERT) ====\n",
    "trainer.train()\n",
    "\n",
    "# ==== 8) Оценка на тестовой выборке ====\n",
    "test_results = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "print(test_results)\n",
    "# {'eval_loss': ..., 'eval_accuracy': ..., 'eval_precision': ..., 'eval_recall': ..., 'eval_f1': ...}\n",
    "\n",
    "# ==== 9) Пример инференса одного текста ====\n",
    "sample_text = \"A new movie starring popular actors is releasing soon.\"\n",
    "# Токенизируем\n",
    "encoded_input = tokenizer([sample_text], padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "# Прогоняем через модель\n",
    "with torch.no_grad():\n",
    "    output = model(**encoded_input)\n",
    "    logits = output.logits\n",
    "    pred_class_id = logits.argmax(dim=1).item()\n",
    "    pred_class_label = id_to_label[pred_class_id]\n",
    "\n",
    "print(\"Исходный текст:\", sample_text)\n",
    "print(\"Предсказанный класс:\", pred_class_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c887c99-7b1f-4dd8-8d9b-d6e08782e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Явное сохранение финальной модели после обучения\n",
    "trainer.save_model(\"distilbert_bbc_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef881e5e-1c07-4255-bbde-122a0c366b71",
   "metadata": {},
   "source": [
    "business:\n",
    "https://www.bbc.com/news/articles/c9djj7pgz57o\n",
    "https://www.bbc.com/news/articles/cvgppr7g508o\n",
    "https://www.bbc.com/news/articles/cgm88w7mjw4o\n",
    "https://www.bbc.com/news/articles/cj0zz357532o\n",
    "https://www.bbc.com/news/videos/cvgnnw15z83o\n",
    "entertainment:\n",
    "https://www.bbc.com/news/articles/c9w8g7jlkpwo\n",
    "https://www.bbc.com/news/articles/cdxgkqp8g10o\n",
    "https://www.bbc.com/news/articles/cm2nxxe238mo\n",
    "https://www.bbc.com/news/articles/c30q87j01yyo\n",
    "https://www.bbc.com/news/articles/cn80zvvy2zvo\n",
    "politics:\n",
    "https://www.bbc.com/news/articles/c7vnz4jy97no\n",
    "https://www.bbc.com/news/articles/cdxqk1x0n5lo\n",
    "https://www.bbc.com/news/articles/clyw8jw11jwo\n",
    "https://www.bbc.com/news/articles/cjd3jr7e4n3o\n",
    "https://www.bbc.com/news/articles/c7vnnv6n29no\n",
    "sport:\n",
    "https://www.bbc.com/sport/football/articles/c20xngdprpyo\n",
    "https://www.bbc.com/sport/tennis/articles/cd7vg2dn39eo\n",
    "https://www.bbc.com/sport/cricket/articles/cn4wgj2v2neo\n",
    "https://www.bbc.com/sport/football/articles/cewggw0q4v7o\n",
    "https://www.bbc.com/sport/rugby-union/articles/ckgr4dkd3pwo\n",
    "tech:\n",
    "https://www.bbc.com/news/articles/cj3xjrj7v78o\n",
    "https://www.bbc.com/news/articles/cg4114271x2o\n",
    "https://www.bbc.com/news/articles/cx28845z30go\n",
    "https://www.bbc.com/future/article/20250404-where-ev-batteries-go-to-die-and-be-reborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f62384b-fa35-4f35-9e44-4d392763019d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры предсказаний на новом датасете:\n",
      "      true_label pred_label_bert  \\\n",
      "0       business        business   \n",
      "1       business        business   \n",
      "2       business        business   \n",
      "3       business        business   \n",
      "4       business        business   \n",
      "5  entertainment   entertainment   \n",
      "6  entertainment   entertainment   \n",
      "7  entertainment   entertainment   \n",
      "8  entertainment   entertainment   \n",
      "9  entertainment   entertainment   \n",
      "\n",
      "                                     pred_probs_bert  \n",
      "0  {'business': 0.99890447, 'entertainment': 0.00...  \n",
      "1  {'business': 0.9988494, 'entertainment': 0.000...  \n",
      "2  {'business': 0.9989189, 'entertainment': 0.000...  \n",
      "3  {'business': 0.9989177, 'entertainment': 0.000...  \n",
      "4  {'business': 0.99885666, 'entertainment': 0.00...  \n",
      "5  {'business': 0.00023044243, 'entertainment': 0...  \n",
      "6  {'business': 0.00022685429, 'entertainment': 0...  \n",
      "7  {'business': 0.00020540925, 'entertainment': 0...  \n",
      "8  {'business': 0.00034295965, 'entertainment': 0...  \n",
      "9  {'business': 0.00020133644, 'entertainment': 0...  \n",
      "\n",
      "Отчет по качеству инференса на новом датасете:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.71      1.00      0.83         5\n",
      "entertainment       1.00      1.00      1.00         5\n",
      "     politics       1.00      0.60      0.75         5\n",
      "        sport       1.00      1.00      1.00         5\n",
      "         tech       0.80      0.80      0.80         5\n",
      "\n",
      "     accuracy                           0.88        25\n",
      "    macro avg       0.90      0.88      0.88        25\n",
      " weighted avg       0.90      0.88      0.88        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Задаем устройство (GPU, если доступно)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Загрузка сохраненной модели и токенизатора\n",
    "# Загрузка модели из папки, куда она была сохранена\n",
    "model_path = \"distilbert_bbc_final\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Загружаем сопоставление id -> label, если оно было сохранено в модели\n",
    "# Если в конфигурации модели уже задан id2label, то используем его\n",
    "id_to_label = model.config.id2label if model.config.id2label is not None else None\n",
    "\n",
    "# Перевод модели в режим инференса и на устройство\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def predict_news_bert(text, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Функция принимает сырой текст, токенизирует его, пропускает через модель,\n",
    "    и возвращает:\n",
    "       - предсказанную метку,\n",
    "       - массив вероятностей (softmax) для каждого класса.\n",
    "    \"\"\"\n",
    "    # Токенизация (с padding и truncation)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=256)\n",
    "    # Переносим входные данные на device\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "        logits = outputs.logits  # размер [1, num_labels]\n",
    "        probs = softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    pred_idx = np.argmax(probs)\n",
    "    if id_to_label is not None:\n",
    "        pred_label = id_to_label[pred_idx]\n",
    "    else:\n",
    "        pred_label = str(pred_idx)\n",
    "    return pred_label, probs\n",
    "\n",
    "# Применяем модель к новому датасету df_new, где колонка \"text\" содержит новости\n",
    "pred_labels = []\n",
    "pred_probs_list = []\n",
    "\n",
    "for text in df_new['text']:\n",
    "    label, probs = predict_news_bert(text, model, tokenizer, device)\n",
    "    pred_labels.append(label)\n",
    "    pred_probs_list.append(probs)\n",
    "\n",
    "# Добавляем результаты инференса к DataFrame\n",
    "df_new['pred_label_bert'] = pred_labels\n",
    "# Преобразуем вероятность в словарь {label: probability} для удобства вывода, если id_to_label задан\n",
    "if id_to_label is not None:\n",
    "    df_new['pred_probs_bert'] = [dict(zip(id_to_label.values(), probs)) for probs in pred_probs_list]\n",
    "else:\n",
    "    df_new['pred_probs_bert'] = [dict(enumerate(probs)) for probs in pred_probs_list]\n",
    "\n",
    "# Выводим первые несколько строк с предсказаниями\n",
    "print(\"Примеры предсказаний на новом датасете:\")\n",
    "print(df_new[['true_label', 'pred_label_bert', 'pred_probs_bert']].head(10))\n",
    "\n",
    "# Если истинные метки присутствуют (столбец \"true_label\"), можно вычислить отчет по метрикам:\n",
    "print(\"\\nОтчет по качеству инференса на новом датасете:\")\n",
    "print(classification_report(df_new['true_label'], df_new['pred_label_bert']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae936b8-c595-451d-8061-83dc137df39c",
   "metadata": {},
   "source": [
    "\n",
    "## Ниже краткий сравнительный анализ результатов двух подходов:\n",
    "\n",
    "**Классический метод (TF-IDF + Logistic Regression):**  \n",
    "- Общая точность составила 76%.  \n",
    "- Модели хорошо справились с классами «entertainment» и «sport» (получены идеальные показатели), однако для классов «business», «politics» и «tech» наблюдаются проблемы. Например, для «business» низкая точность (precision 0.45) при идеальной полноте (recall 1.00) говорит о большом количестве ложноположительных предсказаний, а для «politics» и «tech» f1-меры оказались ниже (~0.57).  \n",
    "- Такой результат может говорить о том, что выбранный классический подход чувствителен к неидеальной предобработке и лексическим особенностям новостного текста, особенно при небольшом размере выборки (всего 5 примеров на класс).\n",
    "\n",
    "**Fine-tuned DistilBERT:**  \n",
    "- Модель показала общую точность 88% с улучшенными макро- и взвешенными метриками (macro avg f1 ≈ 0.88).  \n",
    "- Особенно заметно улучшение для сложных классов: для «business» f1 вырос до 0.83, а для «politics» — до 0.75, что свидетельствует о лучшей способности модели улавливать контекстуальные признаки и нюансы языка.\n",
    "- Повышенные результаты обусловлены возможностями трансформерной архитектуры извлекать глубокие семантические зависимости из текста, что особенно важно при разнообразии формулировок в новостях.\n",
    "\n",
    "**Вывод:**  \n",
    "Fine-tuned DistilBERT демонстрирует явное преимущество по качеству инференса: модель более стабильно и точно предсказывает классы новостей за счет использования контекстных эмбеддингов, чего не достигает классический алгоритм на основе TF-IDF и Logistic Regression. При этом для реальных задач, особенно когда доступно достаточное количество данных и требуются тонкие различия между категориями, модели на базе трансформеров оказываются более эффективными, несмотря на увеличение вычислительных затрат на обучение и инференс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef25c7f7-9c3e-4fd4-a28f-06e129e4e4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
